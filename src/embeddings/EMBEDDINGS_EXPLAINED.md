# Graph Embeddings Explained: Node2Vec + Visualisering

Denna guide f√∂rklarar hur vi anv√§nder **Node2Vec** f√∂r att skapa vektorrepresentationer av produkter och hur vi visualiserar dessa i 2D och 3D.

---

## üìö Inneh√•ll

1. [√ñversikt](#√∂versikt)
2. [Steg 1: Grafen (r√•data)](#steg-1-grafen-r√•data)
3. [Steg 2: Node2Vec skapar embeddings](#steg-2-node2vec-skapar-embeddings)
4. [Steg 3: Visualisering med t-SNE](#steg-3-visualisering-med-t-sne)
5. [Vad √§r Component 1, 2, 3?](#vad-√§r-component-1-2-3)
6. [Alternativ: Weight-baserad visualisering](#alternativ-weight-baserad-visualisering)
7. [Anv√§ndning](#anv√§ndning)
8. [F√∂rdelar och begr√§nsningar](#f√∂rdelar-och-begr√§nsningar)

---

## √ñversikt

### Problem
Vi har en graf med 1000+ produkter och 90,000+ kopplingar. Hur kan vi:
- Hitta liknande produkter snabbt?
- Visualisera produktrelationer?
- Kvantifiera produktlikhet?

### L√∂sning
**Node2Vec** ‚Üí skapar 64-dimensionella vektorer f√∂r varje produkt
**Cosine similarity** ‚Üí m√§ter likhet mellan produkter
**t-SNE/PCA** ‚Üí reducerar till 2D/3D f√∂r visualisering

---

## Steg 1: Grafen (r√•data)

V√•r flavor graph inneh√•ller:
- **1023 produkter** (noder)
- **90,104 kopplingar** (edges)

Varje koppling har en vikt baserad p√•:

```python
Weight = ingredient_match √ó 1.5 +  # Gemensamma ingredienser
         user_match √ó 0.6 +         # Co-purchase data
         tag_match √ó 1.0            # Gemensamma taggar/kategorier
```

### Exempel p√• graf-struktur:

```
[Coca-Cola Original] --12.5--> [Coca-Cola Zero]
[Coca-Cola Original] --11.1--> [Fanta]
[Coca-Cola Original] --8.3---> [Sprite]
[Snickers]           --9.1---> [Mars]
[Snickers]           --7.5---> [Twix]
```

Starkare kopplingar = mer liknande produkter.

---

## Steg 2: Node2Vec skapar embeddings

Node2Vec konverterar grafstrukturen till vektorer genom tv√• steg:

### 2a. Random Walks p√• grafen

Node2Vec "promenerar" runt i grafen och skapar sekvenser av produkter:

```
Walk 1: Coca-Cola ‚Üí Fanta ‚Üí Sprite ‚Üí 7UP ‚Üí Pepsi
Walk 2: Coca-Cola ‚Üí Sprite ‚Üí Fanta ‚Üí Coca-Cola Zero ‚Üí Fanta
Walk 3: Snickers ‚Üí Mars ‚Üí Twix ‚Üí Bounty ‚Üí Kit Kat ‚Üí Snickers
Walk 4: Vitamin Well Defence ‚Üí Vitamin Well Awake ‚Üí Vitamin Well Reload
...
```

**Parametrar:**
- `num_walks = 200` ‚Üí 200 walks per produkt
- `walk_length = 30` ‚Üí varje walk √§r 30 steg l√•ng
- `p = 1.0` (return parameter) ‚Üí styr sannolikhet att √•terv√§nda till f√∂reg√•ende nod
- `q = 1.0` (in-out parameter) ‚Üí styr exploration vs exploitation
  - `q > 1`: h√•ll n√§ra startnod (BFS-liknande, lokal struktur)
  - `q < 1`: utforska l√•ngt (DFS-liknande, global struktur)

**Edge weights p√•verkar walks:**
- Kopplingar med h√∂gre weight ‚Üí h√∂gre sannolikhet att v√§ljas
- Detta betyder att produkter med starka relationer oftare "ses tillsammans"

### 2b. Word2Vec l√§r embeddings

Node2Vec behandar walks som "meningar" och produkter som "ord":

```python
# Walks blir "meningar":
sentences = [
    ["Coca-Cola", "Fanta", "Sprite", "7UP", "Pepsi"],
    ["Snickers", "Mars", "Twix", "Bounty", "Kit Kat"],
    ...
]

# Word2Vec (Skip-gram) l√§r:
# "Om Coca-Cola ofta f√∂rekommer n√§ra Fanta i walks,
#  ska deras vektorer vara liknande"
```

**Word2Vec objective:**
Maximera sannolikheten att f√∂ruts√§ga kontext (grannar) fr√•n m√•lprodukt:

```
maximize Œ£ log P(neighbor | product)
```

**Resultat:** Varje produkt f√•r en **64-dimensionell vektor**:

```python
embeddings = {
    "Coca-Cola":    [0.23, -0.45, 0.12, ..., 0.67],  # 64 v√§rden
    "Fanta":        [0.19, -0.42, 0.15, ..., 0.71],  # 64 v√§rden
    "Snickers":     [-0.82, 0.31, -0.19, ..., 0.02], # 64 v√§rden
    "Vitamin Well": [0.54, 0.22, -0.31, ..., 0.19],  # 64 v√§rden
}
```

**Vad f√•ngar embeddings?**
- Produkter som ofta "ses tillsammans" i walks ‚Üí liknande vektorer
- Produkter i samma grannskap ‚Üí liknande vektorer
- Grafstruktur och communities ‚Üí kodade i vektorerna

---

## Steg 3: Visualisering med t-SNE

Nu har vi 64 dimensioner men kan bara visualisera 2D eller 3D. **t-SNE** (t-Distributed Stochastic Neighbor Embedding) komprimerar dimensionerna.

### Hur t-SNE fungerar:

#### 1. Ber√§kna "n√§rhet" i 64D

F√∂r varje produktpar, ber√§kna euklidiskt avst√•nd:

```
Avst√•nd(Coca-Cola, Fanta)    = 0.15 (n√§ra!)
Avst√•nd(Coca-Cola, Snickers) = 2.87 (l√•ngt!)
```

Konvertera avst√•nd till sannolikheter (n√§rmare = h√∂gre sannolikhet):

```
P(Fanta | Coca-Cola)    = 0.85  (h√∂g sannolikhet att vara granne)
P(Snickers | Coca-Cola) = 0.03  (l√•g sannolikhet)
```

#### 2. Skapa 3D-representation

t-SNE placerar punkter i 3D s√• att:
- Produkter **n√§ra i 64D** f√∂rblir **n√§ra i 3D**
- Produkter **l√•ngt ifr√•n i 64D** f√∂rblir **l√•ngt ifr√•n i 3D**

#### 3. Optimeringsprocess

t-SNE anv√§nder gradient descent f√∂r att minimera Kullback-Leibler divergence:

```
minimize KL(P_64D || P_3D)

d√§r:
P_64D = sannolikhetsf√∂rdelning f√∂r n√§rhet i 64 dimensioner
P_3D  = sannolikhetsf√∂rdelning f√∂r n√§rhet i 3 dimensioner
```

Iterativ process:
```
Iteration 1:   Slumpm√§ssiga positioner i 3D
Iteration 50:  Stora kluster b√∂rjar bildas
Iteration 200: Finjustering av positioner
Iteration 300: Konvergerat! ‚úì
```

#### 4. Resultat

Varje produkt f√•r en 3D-koordinat:

```python
3D_positions = {
    "Coca-Cola":    [12.3, -5.7,  8.1],   # Component 1, 2, 3
    "Fanta":        [11.8, -6.2,  7.9],   # N√§ra Coca-Cola!
    "Snickers":     [-15.2, 3.4, -2.1],   # L√•ngt fr√•n Coca-Cola!
    "Vitamin Well": [-8.4,  15.3, -3.2],  # Egen cluster
}
```

---

## Vad √§r Component 1, 2, 3?

Component 1, 2, 3 √§r **nya, konstgjorda axlar** som t-SNE skapar.

### Analogi

T√§nk dig att du har:
- **64D-vektor** = en beskrivning av produkten med 64 egenskaper
  - Egenskap 1 = hur mycket citrussmak
  - Egenskap 2 = hur mycket kolsyra
  - Egenskap 3 = hur s√∂t
  - ... (61 fler egenskaper)

t-SNE s√§ger: 
> *"Ist√§llet f√∂r 64 egenskaper, kan jag skapa 3 nya 'super-egenskaper' som f√•ngar det viktigaste"*

**Men**: Dessa super-egenskaper √§r **inte tolkbara**. De √§r matematiska kombinationer av alla 64 original-dimensioner.

### Varf√∂r inte tolkbara?

t-SNE √§r en **icke-linj√§r** transformation. Det betyder:

```
Component 1 ‚â† "ingrediens similarity"
Component 1 ‚â† "pris"
Component 1 ‚â† "popularitet"
Component 1 = ü§∑‚Äç‚ôÇÔ∏è "en komplex, icke-linj√§r funktion av alla 64 dimensioner"
```

### Vad kan du s√§ga om komponenterna?

Det enda du kan tolka √§r **relationer**:
- **N√§rhet i 3D** = liknande produkter (i grafstrukturen)
- **Kluster** = grupper av produkter som h√§nger ihop
- **Avst√•nd** = hur olika produkter √§r
- **Riktning** = betyder ingenting specifikt

### Exempel p√• vad du INTE kan s√§ga:

‚ùå "Produkter med h√∂gt Component 1-v√§rde √§r s√∂ta"
‚ùå "Component 2 representerar pris"
‚ùå "Component 3 √§r kolsyreinneh√•ll"

### Exempel p√• vad du KAN s√§ga:

‚úÖ "Dessa tv√• produkter √§r n√§ra varandra ‚Üí de √§r strukturellt lika i grafen"
‚úÖ "Detta kluster inneh√•ller bara l√§skedrycker ‚Üí embeddings f√•ngar kategori"
‚úÖ "Denna produkt √§r outlier ‚Üí den har unika relationer"

---

## Alternativ: Weight-baserad visualisering

F√∂r **tolkbara axlar**, anv√§nd `--visualize-weights` ist√§llet!

### Tolkbara dimensioner

Ist√§llet f√∂r abstrakta komponenter, anv√§nder vi **faktiska graph weights**:

```python
# F√∂r varje produkt, ber√§kna genomsnittliga weights med alla grannar:

X-axel = Average ingredient_match
Y-axel = Average user_match (co-purchase)
Z-axel = Average tag_match
```

### Exempel:

```
Coca-Cola Original:
  - Avg ingredient_match: 3.2  (medelh√∂g, delar ingredienser med flera)
  - Avg user_match:      4.8  (h√∂g, k√∂ps ofta tillsammans med andra)
  - Avg tag_match:       2.1  (l√•g, unik i sin kategori)
  
Position: [3.2, 4.8, 2.1]
```

### F√∂rdelar med weight-visualisering:

‚úÖ **Direkt tolkbara axlar**
- H√∂gt X-v√§rde ‚Üí m√•nga gemensamma ingredienser
- H√∂gt Y-v√§rde ‚Üí ofta k√∂ps tillsammans med andra
- H√∂gt Z-v√§rde ‚Üí central i sin kategori

‚úÖ **Insikter om produktegenskaper**
- Produkt l√§ngst till h√∂ger ‚Üí ingrediensm√§ssigt popul√§r
- Produkt l√§ngst upp ‚Üí stark cross-selling potential
- Produkt i mitten ‚Üí genomsnittlig p√• alla dimensioner

‚úÖ **Anv√§ndbart f√∂r strategi**
- Hitta produkter med h√∂g co-purchase ‚Üí bundle-rekommendationer
- Hitta produkter med h√∂g ingredient similarity ‚Üí substitut
- Hitta produkter med l√•g tag_match ‚Üí nisch-produkter

### t-SNE vs Weight-visualisering

| Aspekt | t-SNE | Weight-baserad |
|--------|-------|----------------|
| **Axlar** | Abstrakta komponenter | ingredient/user/tag weights |
| **Tolkbarhet** | Nej | Ja |
| **Dimensioner** | Alla 64 komprimerade | Bara 3 av m√•nga m√∂jliga |
| **Bra f√∂r** | Se kluster och struktur | F√∂rst√• varf√∂r produkter √§r lika |
| **Anv√§ndning** | Explorativ analys | Strategiska beslut |

**Rekommendation:** Anv√§nd b√•da!
- **t-SNE** f√∂r att uppt√§cka m√∂nster
- **Weight-visualisering** f√∂r att f√∂rst√• varf√∂r m√∂nstren existerar

---

## Anv√§ndning

### Grundl√§ggande produkts√∂kning

```bash
# Visa tillg√§ngliga produkter
python find_similar_products.py

# S√∂k efter liknande produkter
python find_similar_products.py --product-name "Coca-Cola" --topn 10

# S√∂k med produkt-ID
python find_similar_products.py --product-id "07310350118342" --topn 5
```

### Visualiseringar

```bash
# 2D t-SNE (f√§rgkodad efter subcategory)
python find_similar_products.py --visualize

# 3D t-SNE (interaktiv, √∂ppnas i webbl√§sare)
python find_similar_products.py --visualize-3d

# 3D weight-baserad (tolkbara axlar!)
python find_similar_products.py --visualize-weights

# V√§lj PCA ist√§llet f√∂r t-SNE
python find_similar_products.py --visualize --vis-method pca
```

### Tr√§na om embeddings

```bash
# Tr√§na om med nya parametrar
python find_similar_products.py --retrain --dimensions 128

# Tr√§na och visualisera direkt
python find_similar_products.py --retrain --visualize-3d
```

### Kombinera funktioner

```bash
# S√∂k efter produkt och visualisera
python find_similar_products.py --product-name "Snickers" --visualize-weights
```

---

## F√∂rdelar och begr√§nsningar

### ‚úÖ F√∂rdelar med Node2Vec

1. **Snabb similarity search**
   - Cosine similarity i vektorrymd: O(n) ist√§llet f√∂r O(n¬≤) graftraversering
   - Kan anv√§nda FAISS eller andra ANN-bibliotek f√∂r miljontals produkter

2. **F√•ngar grafstruktur**
   - Direkta kopplingar (grannar)
   - Indirekta kopplingar (grannar-av-grannar)
   - Community structure (kluster)

3. **Flexibel**
   - Kan anv√§ndas f√∂r olika downstream tasks
   - Rekommendationer, clustering, anomaly detection

4. **Skalbar**
   - Tr√§ning kan paralleliseras
   - Embeddings kan cachas och √•teranv√§ndas

### ‚ö†Ô∏è Begr√§nsningar

1. **Tr√§ning tar tid**
   - F√∂r 1000 produkter: ~1-2 minuter
   - F√∂r 100,000 produkter: flera timmar

2. **Hyperparametrar**
   - `p`, `q`, `walk_length`, `num_walks` p√•verkar resultat
   - Kr√§ver experimentering f√∂r optimala v√§rden

3. **Statiska embeddings**
   - Om grafen uppdateras m√•ste embeddings tr√§nas om
   - Nya produkter kr√§ver ny tr√§ning (eller approximation)

4. **Interpretability**
   - Sv√•rt att f√∂rst√• vad varje dimension betyder
   - "Black box" f√∂r icke-tekniska anv√§ndare
   - ‚Üí L√∂sning: Anv√§nd weight-visualisering ist√§llet!

---

## Tekniska detaljer

### Node2Vec-parametrar

```python
node2vec = Node2Vec(
    graph=G,
    dimensions=64,        # Vektor-storlek (32-256 √§r vanligt)
    walk_length=30,       # L√§ngd p√• random walks
    num_walks=200,        # Antal walks per nod
    p=1.0,                # Return parameter (BFS vs DFS)
    q=1.0,                # In-out parameter
    workers=4,            # Parallellisering
    weight_key='weight'   # Anv√§nd edge weights
)

model = node2vec.fit(
    window=10,            # Context window f√∂r Word2Vec
    min_count=1,          # Minimum word frequency
    batch_words=4,        # Batch size f√∂r training
    epochs=10             # Antal training epochs
)
```

### t-SNE-parametrar

```python
tsne = TSNE(
    n_components=3,       # 2D eller 3D
    perplexity=30,        # Balans mellan lokal och global struktur
    random_state=42,      # F√∂r reproducerbarhet
    n_iter=1000,          # Antal iterationer
    learning_rate=200     # Gradient descent step size
)
```

### Similarity-ber√§kning

```python
# Cosine similarity mellan tv√• produkter
similarity = np.dot(vector1, vector2) / (
    np.linalg.norm(vector1) * np.linalg.norm(vector2)
)

# V√§rden: 0 (ortogonala) till 1 (identiska)
```

---

## Vidare l√§sning

### Node2Vec
- [Original paper: node2vec - Scalable Feature Learning for Networks](https://arxiv.org/abs/1607.00653)
- [node2vec Python library](https://github.com/eliorc/node2vec)

### t-SNE
- [Original paper: Visualizing Data using t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)
- [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)

### Word2Vec
- [Original paper: Efficient Estimation of Word Representations](https://arxiv.org/abs/1301.3781)
- [Word2Vec Tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)

### Graph Embeddings
- [Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/)
- [Stanford CS224W: Machine Learning with Graphs](http://web.stanford.edu/class/cs224w/)

---

## Fr√•gor och svar

### Q: Varf√∂r 64 dimensioner?
**A:** Balans mellan:
- Expressiveness (fler dimensioner = mer information)
- Computation (f√§rre dimensioner = snabbare)
- Overfitting (f√∂r m√•nga dimensioner = memorerar grafen)

Typiska v√§rden: 32, 64, 128, 256

### Q: Kan jag lita p√• similarity scores?
**A:** Ja, f√∂r relativa j√§mf√∂relser:
- Score 0.95 vs 0.60 ‚Üí f√∂rsta produkten √§r mycket mer lik
- Absoluta v√§rden √§r mindre viktiga

### Q: Hur ofta ska jag tr√§na om?
**A:** N√§r grafen √§ndras signifikant:
- Nya produkter tillkommer
- Edge weights uppdateras med ny f√∂rs√§ljningsdata
- Graph struktur f√∂r√§ndras

### Q: Kan jag anv√§nda embeddings f√∂r andra uppgifter?
**A:** Ja! Embeddings kan anv√§ndas f√∂r:
- Clustering (gruppera liknande produkter)
- Classification (f√∂ruts√§ga kategori)
- Link prediction (f√∂ruts√§ga framtida kopplingar)
- Anomaly detection (hitta ovanliga produkter)

### Q: Varf√∂r Node2Vec ist√§llet f√∂r andra metoder?
**A:** Alternativen:
- **DeepWalk**: Enklare, men anv√§nder inte edge weights
- **LINE**: Snabbare, men bara f√∂r f√∂rsta och andra ordningens proximitet
- **GraphSAGE/GCN**: Kraftfullare, men kr√§ver mer data och ber√§kning
- **Node2Vec**: Bra balans mellan prestanda, flexibilitet och enkelhet

---

## Kontakt

F√∂r fr√•gor eller feedback, kontakta projektutvecklarna eller √∂ppna ett issue p√• GitHub.

**Lycka till med dina graph embeddings!** üöÄ
